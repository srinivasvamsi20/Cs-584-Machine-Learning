# -*- coding: utf-8 -*-
"""ML_Project.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1qYnr6wux17YF6PmZs--SWxUlIgy-ja9k
"""

# Commented out IPython magic to ensure Python compatibility.
# %matplotlib inline
!pip install surprise
import matplotlib.pyplot as plt
import seaborn as sns
import pandas as pd
import numpy as np
import warnings
from surprise import SVD, Reader
from surprise import Dataset
from scipy import stats
from ast import literal_eval
from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer
from sklearn.metrics.pairwise import linear_kernel, cosine_similarity
from nltk.stem.snowball import SnowballStemmer
from nltk.stem.wordnet import WordNetLemmatizer
from nltk.corpus import wordnet
from surprise.model_selection import cross_validate

def load_data():

    movie_credits_df       = pd.read_csv('credits.csv')
    movie_keywords_df      = pd.read_csv('keywords.csv')
    movie_links_small_df   = pd.read_csv('links_small.csv')
    movie_metadata_df      = pd.read_csv('movies_metadata.csv')
    movie_ratings_small_df = pd.read_csv('ratings_small.csv')
    return movie_credits_df, movie_keywords_df, movie_links_small_df, movie_metadata_df, movie_ratings_small_df

"""The load_data() function in Python reads data from five CSV files (‘credits.csv’, ‘keywords.csv’, ‘links_small.csv’, ‘movies_metadata.csv’, ‘ratings_small.csv’) and stores them in respective pandas DataFrames. It then returns these DataFrames. The function assumes the files are in the same directory as the script and requires the pandas library."""

def process_genres(movie_metadata_df):

    movie_metadata_df['genres'] = movie_metadata_df['genres'].fillna('[]').apply(literal_eval).apply(lambda x: [i['name'] for i in x] if isinstance(x, list) else [])
    return movie_metadata_df

def calculate_constants(movie_metadata_df):

    vote_count = movie_metadata_df[movie_metadata_df['vote_count'].notnull()]['vote_count'].astype('int')
    vote_average = movie_metadata_df[movie_metadata_df['vote_average'].notnull()]['vote_average'].astype('int')


    a = vote_average.mean()
    b = vote_count.quantile(0.95)
    return a, b

movie_credits_df, movie_keywords_df, movie_links_small_df, movie_metadata_df, movie_ratings_small_df = load_data()
movie_metadata_df = process_genres(movie_metadata_df)
a, b = calculate_constants(movie_metadata_df)

a, b

"""Average Rating (a):

The calculated average rating is approximately 5.24, reflecting the central tendency of movie ratings in the dataset.

User Engagement Criterion (b):

The threshold of 434.0 for vote count ensures a focus on movies with a significant level of popularity, contributing to the system's accuracy.

Rating Scale Context:

The discrete rating scale, with 5.24 as the average, provides a context for interpreting user sentiments within the recommendation algorithm.
"""

movie_metadata_df['year'] = pd.to_datetime(movie_metadata_df['release_date'], errors='coerce').dt.year

filtered_movies = movie_metadata_df[(movie_metadata_df['vote_count'] >= b) &
                                     movie_metadata_df['vote_count'].notnull() &
                                     movie_metadata_df['vote_average'].notnull()]

selected_columns = ['title', 'year', 'vote_count', 'vote_average', 'popularity', 'genres']
filtered_movies = filtered_movies[selected_columns]

filtered_movies['vote_count'] = filtered_movies['vote_count'].astype('int')
filtered_movies['vote_average'] = filtered_movies['vote_average'].astype('int')

filtered_movies.shape

"""Data Size:

The output shape (2274, 6) indicates that the dataset now contains 2274 movies with 6 relevant attributes. This filtered subset likely includes movies with a significant vote count and non-null values for vote average.
Year Extraction:

The 'year' column, derived from the 'release_date,' provides a temporal dimension to the dataset, allowing for analyses based on the release year of movies.
Popularity and Genres:

The selected attributes, including 'popularity' and 'genres,' suggest that the filtered dataset retains information about movie popularity and genre composition. This refined dataset can be valuable for building a recommendation system focused on well-received and genre-specific movies
"""

filtered_movies['weighted_rating'] = filtered_movies.apply(
    lambda x: (x['vote_count'] / (x['vote_count'] + b) * x['vote_average']) + (b / (b + x['vote_count']) * a),
    axis=1
)

top_250_movies = filtered_movies.sort_values('weighted_rating', ascending=False).head(250)
top_250_movies.head(5)

"""Genre Expansion:

The code expands the 'genres' column in the movie_metadata DataFrame, transforming it into individual rows for each genre associated with a movie.
Granular Data Representation:

By using stack() and reset_index(), the code achieves a more detailed representation of movie genres, facilitating genre-specific analyses in the resulting general_metadata DataFrame.
Enhanced Data Structure:

The transformation improves the data structure for genre-related information, allowing for easier genre-based exploration and analysis within the movie metadata
"""

transformed_genres = movie_metadata_df.apply(lambda row: pd.Series(row['genres']), axis=1).stack().reset_index(level=1, drop=True)
transformed_genres.name = 'movie_genre'
optimized_metadata = movie_metadata_df.drop('genres', axis=1).join(transformed_genres)
optimized_metadata.head(3).transpose()

"""The below code defines is used for generating movie recommendations based on a specified genre.

Genre-Based Filtering:

The function filters movies in the general_metadata DataFrame based on a specified genre, creating a subset of data for genre-specific analysis.
Weighted Rating Calculation:

Utilizing a weighted rating formula, the function calculates ratings for the filtered movies, considering both vote count and average. This approach prioritizes movies with higher engagement.
Top Recommendations:

The function sorts and selects the top 250 movies based on the calculated weighted ratings, providing personalized recommendations for the specified genre.
"""

print(optimized_metadata.columns)

def genre_recommendation(genre, percentile=0.85):
    data_frames = optimized_metadata[optimized_metadata['movie_genre'] == genre]
    vote_counts = data_frames['vote_count'].dropna().astype('int')
    vote_averages = data_frames['vote_average'].dropna().astype('int')

    C = vote_averages.mean()
    m = vote_counts.quantile(percentile)

    recommendations = data_frames[(data_frames['vote_count'] >= m) &
                                  data_frames['vote_count'].notnull() &
                                  data_frames['vote_average'].notnull()][['title', 'year', 'vote_count', 'vote_average', 'popularity']]

    recommendations['vote_count'] = recommendations['vote_count'].astype('int')
    recommendations['vote_average'] = recommendations['vote_average'].astype('int')

    recommendations['weighted_rating'] = recommendations.apply(lambda x:
                        (x['vote_count']/(x['vote_count']+m) * x['vote_average']) + (m/(m+x['vote_count']) * C),
                        axis=1)

    recommendations = recommendations.sort_values('weighted_rating', ascending=False).head(250)

    return recommendations

genre_recommendation('Romance').head(5)

"""Content based recommendation system"""

print(movie_links_small_df.columns)

print(movie_links_small_df)

"""This code links TMDb IDs from the 'tmdbId' column in movie_links_small_df, maps 'id' values to create a new 'tmdbId' column in movie_metadata_df, and identifies rows with null TMDb IDs. Following this, it cleans movie_metadata_df by removing specific rows and filters the dataset to retain movies with TMDb IDs present in the linked set. The resulting DataFrame, filtered_movies, represents a cleaned subset of movie metadata with linked TMDb IDs, excluding specific rows and ensuring data consistency."""

import numpy as np
linked_tmdb_ids = movie_links_small_df[movie_links_small_df['tmdbId'].notnull()]['tmdbId'].astype('int')
movie_metadata_df['tmdbId'] = movie_metadata_df['id'].apply(lambda x: int(x) if x.isdigit() else np.nan)
null_tmdb_ids = movie_metadata_df[movie_metadata_df['tmdbId'].isnull()]

null_tmdb_ids

movie_metadata_cleaned = movie_metadata_df.drop([19730, 29503, 35587])
movie_metadata_cleaned['id'] = movie_metadata_cleaned['id'].astype('int')
filtered_movies = movie_metadata_cleaned[movie_metadata_cleaned['id'].isin(movie_links_small_df['tmdbId'])]

filtered_movies.shape

"""Inference :Linked TMDb IDs:

The resulting DataFrame, filtered_movies, has 9,099 rows and 26 columns, indicating successful linking of TMDb IDs from movie_links_small_df to the movie_metadata_df.
Data Consistency:

The code has effectively cleaned and filtered the movie metadata, ensuring data consistency by excluding specific rows and mapping 'id' values to create a new 'tmdbId' column.
Subset Representation:

The output shape of (9099, 26) implies that the resulting DataFrame, filtered_movies, represents a subset of movie metadata with linked TMDb IDs, providing a refined dataset for further analysis.

This code is designed to process movie descriptions and taglines in the DataFrame small_mov by combining them into a new 'description' column and filling missing values. It then utilizes TF-IDF vectorization with a word and bigram analyzer to create a matrix representation of the textual data. The resulting cosine similarity matrix, calculated using linear kernel, quantifies the similarity between movie descriptions, serving as a foundation for content-based recommendation systems.

The code is used to enhance movie content analysis, integrating textual information for improved recommendation system capabilities. By leveraging TF-IDF vectorization and cosine similarity, it facilitates the identification of movies with similar descriptions, enriching the dataset for content-based recommendation models.
"""

filtered_movies

import pandas as pd
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import linear_kernel

# Assuming filtered_movies is the DataFrame
filtered_movies['tagline'] = filtered_movies['tagline'].fillna('')
filtered_movies['description'] = filtered_movies['overview'] + filtered_movies['tagline']
filtered_movies['description'] = filtered_movies['description'].fillna('')

tf_vector = TfidfVectorizer(analyzer='word', ngram_range=(1, 2), min_df=0, stop_words='english')
tfidf_vect_matrix = tf_vector.fit_transform(filtered_movies['description'])
print(tfidf_vect_matrix.shape)

cosine_sim = linear_kernel(tfidf_vect_matrix, tfidf_vect_matrix)
print(cosine_sim[0])

"""Inference:
Matrix Dimensions:

The TF-IDF matrix has a shape of (9099, 268124), indicating that it consists of 9099 rows (representing movies) and 268,124 columns (representing unique word or word combinations in the movie descriptions).
Cosine Similarity:

The computed cosine similarity values provide a pairwise similarity score for each movie in the dataset. The first value, 1.0, represents the movie's self-similarity, while other values indicate the similarity between the first movie and the rest in the dataset.
Sparse Similarities:

The majority of cosine similarity values are close to zero, suggesting sparse similarities between movies in the TF-IDF space. This is expected, as most movies have distinct descriptions, resulting in low similarity scores.
"""

filtered_movies = filtered_movies.reset_index()
movie_titles = filtered_movies['title']
movie_indices = pd.Series(filtered_movies.index, index=filtered_movies['title'])

movie_title = 'Made'
index = movie_indices[movie_title]
similarity_scores = list(enumerate(cosine_sim[index]))
similarity_scores = sorted(similarity_scores, key=lambda x: x[1], reverse=True)
similar_movie_indices = [i[0] for i in similarity_scores[1:6]]  # Adjust the number of recommendations as needed
recommended_movies = movie_titles.iloc[similar_movie_indices]

print(recommended_movies)

filtered_movies = filtered_movies.reset_index()
movie_titles = filtered_movies['title']
movie_indices = pd.Series(filtered_movies.index, index=filtered_movies['title'])

movie_title = 'JFK'
index = movie_indices[movie_title]
similarity_scores = list(enumerate(cosine_sim[index]))
similarity_scores = sorted(similarity_scores, key=lambda x: x[1], reverse=True)
similar_movie_indices = [i[0] for i in similarity_scores[1:6]]  # Adjust the number of recommendations as needed
recommended_movies = movie_titles.iloc[similar_movie_indices]

print(recommended_movies)

"""Content based Recommendation System with movie description, taglines, keywords, cast, director and genres"""

# Assuming movie_metadata_df, movie_credits_df, movie_keywords_df, and movie_links_small_df are the DataFrames
metadata = movie_metadata_df.copy()
metadata['id'] = pd.to_numeric(metadata['id'], errors='coerce')
movie_credits_df['id'] = movie_credits_df['id'].astype('int')
movie_keywords_df['id'] = movie_keywords_df['id'].astype('int')

merged_metadata = metadata.merge(movie_credits_df, on='id').merge(movie_keywords_df, on='id')
sm_movies = merged_metadata[merged_metadata['id'].isin(movie_links_small_df['tmdbId'])]
sm_movies.shape

import pandas as pd
from ast import literal_eval

movies = sm_movies.copy()

movies['cast'] = movies['cast'].apply(literal_eval)
movies['crew'] = movies['crew'].apply(literal_eval)
movies['keywords'] = movies['keywords'].apply(literal_eval)
movies['cast_size'] = movies['cast'].apply(lambda x: len(x))
movies['crew_size'] = movies['crew'].apply(lambda x: len(x))

def extract_director(crew_list):
    for i in crew_list:
        if i['job'] == 'Director':
            return i['name']
    return np.nan

movies['director'] = movies['crew'].apply(extract_director)

movies['cast'] = movies['cast'].apply(lambda x: [i['name'] for i in x] if isinstance(x, list) else [])
movies['cast'] = movies['cast'].apply(lambda x: x[:3] if len(x) >= 3 else x)

movies['keywords'] = movies['keywords'].apply(lambda x: [i['name'] for i in x] if isinstance(x, list) else [])

movies['cast'] = movies['cast'].apply(lambda x: [str.lower(i.replace(" ", "")) for i in x])
movies['director'] = movies['director'].astype('str').apply(lambda x: str.lower(x.replace(" ", "")))
movies['director'] = movies['director'].apply(lambda x: [x, x, x])

keywords_series = movies.apply(lambda x: pd.Series(x['keywords']), axis=1).stack().reset_index(level=1, drop=True)
keywords_series.name = 'keyword'
keywords_count = keywords_series.value_counts()

top_keywords = keywords_count[:5]

top_keywords

filtered_keywords_count = keywords_count[keywords_count > 1]
def filter_words(x):
    return [word for word in x if word in filtered_keywords_count]
stemmed_word = [SnowballStemmer('english').stem('dogs')]

print(filtered_keywords_count)
print(filter_words(['dogs', 'cats', 'birds']))
print(stemmed_word)

import pandas as pd
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.metrics.pairwise import cosine_similarity
from nltk.stem.snowball import SnowballStemmer

stemmer = SnowballStemmer("english")
stemmer.stem('dogs')

movies['keywords'] = movies['keywords'].apply(filter_words)
movies['keywords'] = movies['keywords'].apply(lambda x: [stemmer.stem(i) for i in x])
movies['keywords'] = movies['keywords'].apply(lambda x: [str.lower(i.replace(" ", "")) for i in x])
movies['soup'] = movies['keywords'] + movies['cast'] + movies['director'] + movies['genres']
movies['soup'] = movies['soup'].apply(lambda x: ' '.join(x))

cv = CountVectorizer(analyzer='word', ngram_range=(1, 2), min_df=0, stop_words='english')
count_vect_matrix = cv.fit_transform(movies['soup'])
cosine_similarity_matrix = cosine_similarity(count_vect_matrix, count_vect_matrix)

movies = movies.reset_index()
movie_titles = movies['title']
movie_indices = pd.Series(movies.index, index=movies['title'])

def recommends(x):
    index = movie_indices[x]
    scr = list(enumerate(cosine_sim[index]))
    scr = sorted(scr, key=lambda x: x[1], reverse=True)
    scr = scr[1:31]
    mi = [i[0] for i in scr]
    return movie_titles.iloc[mi]

recommends('Inception').head(5)

print(recommended_movies)

"""Adding the system with Popularity and Ratings"""

def weighted_rating(x):
    v = x['vote_count']
    R = x['vote_average']
    return (v / (v + b) * R) + (b / (b + v) * a)

def recommends_improve(movie_title):
    movie_index = movie_indices[movie_title]
    similarities = list(enumerate(cosine_similarity_matrix[movie_index]))
    similarities = sorted(similarities, key=lambda x: x[1], reverse=True)
    similarities = similarities[1:26]
    indices = [i[0] for i in similarities]

    similar_movies = movies.iloc[indices][['title', 'vote_count', 'vote_average', 'year']]

    vote_counts = similar_movies[similar_movies['vote_count'].notnull()]['vote_count'].astype('int')
    vote_averages = similar_movies[similar_movies['vote_average'].notnull()]['vote_average'].astype('int')

    C = vote_averages.mean()
    m = vote_counts.quantile(0.60)

    qualified_movies = similar_movies[(similar_movies['vote_count'] >= m) &
                                      (similar_movies['vote_count'].notnull()) &
                                      (similar_movies['vote_average'].notnull())]

    qualified_movies['vote_count'] = qualified_movies['vote_count'].astype('int')
    qualified_movies['vote_average'] = qualified_movies['vote_average'].astype('int')

    qualified_movies['wr'] = qualified_movies.apply(weighted_rating, axis=1)
    qualified_movies = qualified_movies.sort_values('wr', ascending=False).head(10)

    return qualified_movies

recommends_improve('Interstellar')

"""Hybrid recommendation system"""

ratings_reader = Reader()
ratings_data = Dataset.load_from_df(movie_ratings_small_df[['userId', 'movieId', 'rating']], ratings_reader)
svd_algorithm = SVD()
cv_results = cross_validate(svd_algorithm, ratings_data, measures=['RMSE', 'MAE'], cv=5)
cv_results

def convert_to_int(x):
    try:
        return int(x)
    except:
        return np.nan

movies_links = pd.read_csv('links_small.csv')[['movieId', 'tmdbId']]
movies_links['tmdbId'] = movies_links['tmdbId'].apply(convert_to_int)
movies_links.columns = ['movieId', 'id']
movies_links = movies_links.merge(filtered_movies[['title', 'id']], on='id').set_index('title')

movies_mapper = movies_links.set_index('id')

def hybrid_recommendation(user_id, movie_name):
    movie_index = movie_indices[movie_name]
    tmdb_id = movies_links.loc[movie_name]['id']
    movie_id = movies_links.loc[movie_name]['movieId']

    similar_movies_cosine = list(enumerate(cosine_similarity_matrix[int(movie_index)]))
    similar_movies_cosine = sorted(similar_movies_cosine, key=lambda x: x[1], reverse=True)
    similar_movies_cosine = similar_movies_cosine[1:26]
    similar_movies_indices = [i[0] for i in similar_movies_cosine]

    recommended_movies = filtered_movies.iloc[similar_movies_indices][['title', 'vote_count', 'vote_average', 'release_date', 'id']]
    recommended_movies['est'] = recommended_movies['id'].apply(lambda x: svd_algorithm.predict(user_id, movies_mapper.loc[x]['movieId']).est)
    recommended_movies = recommended_movies.sort_values('est', ascending=False)

    return recommended_movies.head(10)

hybrid_recommendation(1, 'Aliens')